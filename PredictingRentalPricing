---
title: 'Predicting Prices of VRBO Rental Properties Using Ridge Regression, Lasso Method, and Elastic Net'
author: "Charlotte Steinhardt"
date: "November 3, 2022"
output:
  html_document: default
  pdf_document: default
---

```{r include=FALSE}
library(readr)
library(dplyr)
library(glmnet)
library(ggplot2)
library(kableExtra)
library(magrittr)
```

```{r include=FALSE}
VRBO <- read_csv("VRBO.csv")
```


## Executive Summary

The aim of this report is to develop a model that can be used by the client to predict the nightly price of a rental property. Accurate pricing prediction would enable rental property owners to decide how to price new properties such that they are likeliest to be successfully rented out. This report analyzes twelve features (based on customer ratings, architectural configuration and capacity, location, and transportation-related metrics) of over 1,500 rental properties listed on vacation rental site VRBO. The report then constructs three possible models for predicting the nightly price of a rental property. Each model uses a different method: ridge regression, the lasso method, and finally the elastic net method. We compare the three models' predictive performance based on root mean squared error and conclude that the model that most accurately predicts rental properties' nightly price (though not by a wide margin) is the model constructed using the elastic net method.


## Part 1: Introduction

This report analyzes data generated by vacation rental company VRBO. The data set contains 1,561 observations and 13 features. Each observation corresponds to a short-term rental property, about which the data set contains the following information: the unique unit number of the rental listing; the price in US dollars of a one-night rental; an average satisfaction score (ranging from 2.5 to 5); the number of VRBO website reviews of the property; the type of room to be rented (entire property, private room, or shared room); the number of guests the rental can accommodate; the number of bedrooms included in the price; the minimum number of nights an individual is required to book at the property in order to stay there; the names of the neighborhood and of the district in which the property is located; scores for ease and safety of walking, biking, and public transit (appearing to range from 0 to 100); and the proportion of rentals relative to permanent residences in the property's neighborhood.

The client wishes to obtain a model using these features that can predict the nightly price of a rental property. Accurate pricing prediction would enable property owners to decide how to price new rental properties such that they are likeliest to be successfully rented out. This report provides three possible models for predicting the nightly price of a rental property, each using a different method: first ridge regression (Part 3), then the lasso method (Part 4), and finally the elastic net method (Part 5). After explaining the methods and constructing the three models, we compare their predictive performance using test root mean squared error as a metric (the reasoning for which is explained in detail in Part 3) and conclude that the model that most accurately predicts rental properties' nightly price, though not by a wide margin, is the model constructed using the elastic net method. 


## Part 2: Data Cleaning

This section summarizes the steps taken to prepare the data set for analysis: first, discovering and addressing missing information; and second, making some observations concerning the response values, in order to contextualize our interpretations of the models' predictive accuracy.

The VRBO dataset is missing 138 entries, all concerning the minimum number of nights required for booking at a property. From this we conclude that 138 properties do not require customers to book a minimum number of nights, so we replaced each missing entry with 0 to account for zero minimum nights required. 

All of the features in the data set might conceivably inform most customers' decision to book a rental except the unit number (which appears to be a VRBO indexing number rather than an address number), so we exclude that feature when constructing models.

In keeping with the client's preferences, we build models with all features included. Therefore we have not included the analysis of the relationships between each feature and the response variable that would inform a decision to omit a feature, nor have we transformed the data.

Figure 2.1 shows the distribution in the data set of the variable we aim to predict: that is, numbers of rental properties with particular nightly prices in the data set. We see a right-skewed distribution with most properties priced between 0 and 200 USD. The mean price of a nightly rental is 109.50 USD, and the "middle" half of the properties in the data set are priced between 55 and 130 USD per night. 

```{r eval=FALSE, include=FALSE}
# Check to see whether there is any missing data 
sum(is.na(VRBO))

# Determine which data are missing, and quantity
for (i in 1:14) {
  missing <- sum(is.na(VRBO[,i]))
  if(missing > 0){
    print(colnames(VRBO[,i]))
    print(sum(is.na(VRBO[,i])))
}
}
```

```{r include=FALSE}
# Convert NAs into 0
VRBO[is.na(VRBO)] <- 0

# Dimensions of data?
dim(VRBO)
```

```{r include=FALSE}
#Rename variables so that they look nice in tables, 
VRBO <- VRBO %>% dplyr::select(Price = price, OverallSatisfaction = overall_satisfaction, Reviews = reviews, RoomType = room_type, PersonsAccommodated = accommodates, NumberBedrooms = bedrooms, MinimumStay = minstay, Neighorhood = neighborhood, District = district, WalkScore, TransitScore, BikeScore, PercentRentals = PctRentals)
#Note: Must specify dplyr-select function rather than mass-select function to avoid "unused argument" error.
```

```{r eval=FALSE, include=FALSE}
# Confirm correct variable classification
VRBO$Price <- as.numeric(VRBO$Price)
VRBO$OverallSatisfaction <- as.numeric(VRBO$OverallSatisfaction)
VRBO$Reviews <- as.numeric(VRBO$Reviews)
VRBO$RoomType <- as.factor(VRBO$RoomType)
VRBO$PersonsAccommodated <- as.numeric(VRBO$PersonsAccommodated)
VRBO$NumberBedrooms <- as.numeric(VRBO$NumberBedrooms)
VRBO$MinimumStay <- as.numeric(VRBO$MinimumStay)
VRBO$Neighorhood <- as.factor(VRBO$Neighorhood)
VRBO$District <- as.factor(VRBO$District)
VRBO$WalkScore <- as.numeric(VRBO$WalkScore)
VRBO$TransitScore <- as.numeric(VRBO$TransitScore)
VRBO$BikeScore <- as.numeric(VRBO$BikeScore)
VRBO$PercentRentals <- as.numeric(VRBO$PercentRentals)
```

```{r eval=FALSE, include=FALSE}
summary(VRBO)
```

```{r echo=FALSE}
ggplot(data = VRBO, aes(x = Price)) + 
  geom_histogram(binwidth = 5, fill = "tomato") +
  labs(title = "Figure 2.1: Distribution of VRBO Rental Prices", x = "Price (USD) nightly", y = "Number of rentals with given price")
```

```{r eval=FALSE, include=FALSE}
sum(VRBO$Price > 200)
129/1561
```

In the following sections, we construct and compare the three models, after summarizing the motivations and processes involved in ridge regression, the lasso method, and the elastic net method.


## Part 3: Ridge Regression

In this section, we explain the incentive for and process of using ridge regression, then train our first model using this technique.

Ridge regression is especially useful in situations in which we intend to use least squares linear regression (LSLR) but discover that some features in the desired model are too highly correlated for LSLR to work. In LSLR, highly correlated features cause inflation of the model coefficients we estimate (entries in the vector $\boldsymbol{\hat\beta}$), as well as dramatic inflation in the variances of these estimated coefficients, which diminishes the usefulness of the model's predictions. 

We note that, unfortunately, we trade this decrease in variance for an increase in biasedness (distance of an estimate from the true value) of $\boldsymbol{\hat\beta}$. This is true of all three methods employed in this report.

Choosing ridge regression over LSLR is advisable in the case of the VRBO data set. From the (darker-colored) correlation values close to 1 in the matrix shown in Figure 3.1, we see that there is high correlation among some features: for example, between the transit and walk scores and between a property's number of bedrooms and the number of guests it can accommodate.

```{r echo=FALSE, warnings = FALSE, messages = FALSE}
VRBOnum <- VRBO %>% dplyr::select(OverallSatisfaction, Reviews, PersonsAccommodated, NumberBedrooms, MinimumStay, WalkScore, TransitScore, BikeScore, PercentRentals)

corrplot::corrplot(cor(VRBOnum), 
                   method = "number", 
                   tl.cex = .85, 
                   number.cex = 0.70, 
                   tl.col = "black", 
                   title = "Figure 3.1: Correlation Matrix of Numeric VRBO Features",
                   mar = c(0,0,3,0))

# I found the "number.cex = " code on https://statisticsglobe.com/change-font-size-corrplot-r. 
```

Ridge regression retains all of the features we initially choose to include in the model, and copes with any high correlation among them by minimizing the sum of squared errors in addition to a shrinkage penalty term ($\boldsymbol{\lambda_{Ridge}\hat{\beta}^T\hat{\beta}}$) that directly affects the inflated $\boldsymbol{\hat\beta}$ terms. We can control the degree to which we deflate the $\boldsymbol{\hat\beta}$ terms by adjusting the value of the tuning parameter, $\lambda_{Ridge}$. If we increase the value of $\lambda_{Ridge}$, we shrink the $\boldsymbol{\hat\beta}$ coefficients (and associated variance), and if we decrease the value of $\lambda_{Ridge}$, we see larger $\boldsymbol{\hat\beta}$ coefficients.

We consider a range of non-negative values for this tuning parameter $\lambda_{Ridge}$ in order to determine which leads to the lowest root mean squared error (RMSE). The root mean squared error is the root of the sum of squared residuals averaged over $n$ observations ($\sqrt{\frac{1}{n}\Sigma_i^n(y_i-\hat{y_i})^2}$); in other words, it is the estimated standard deviation of the residuals. Using the *square root* of the mean squared error rather than the mean squared error allows us to keep the error units on the same scale as the response variable (in this case, US dollars), which makes the error metric more clearly interpretable.

We begin by considering $\lambda_{Ridge} = 0$ (in which case the penalty term would be $0\times\boldsymbol{\hat{\beta}^T\hat{\beta}}$, and all we would be seeking to minimize would be the sum of squared errors, just as in LSLR). We consider values of $\lambda_{Ridge}$ up to 25 (expanding this range if necessary) in increments of 0.1. We then estimate the coefficients $\boldsymbol{\hat\beta}$ using each value of $\lambda_{Ridge}$, and then use 10-fold cross validation to find the test RMSE for each of those models. 

As shown in Figure 3.2, the smallest RMSE occurs when the tuning parameter $\lambda_{Ridge} =$ 14.9. Figure 3.2 illustrates both the scale of RMSE for $\lambda_{Ridge}$ (close to 61 for the range of values we considered), and that we have in fact found the value of $\lambda_{Ridge}$ that minimizes the RMSE and need not consider further values of $\lambda_{Ridge}$. (The plotted function has exactly one minimum value.)

```{r include=FALSE}
# Code for ridge plot, written by Dr. Dalzell
ridgePlot <- function(ridge.mod, metric, title){
  library(ggplot2)
  
  smallestLambda <- ridge.mod$lambda[which.min(ridge.mod$cvm)] 
  
  if(metric == "MSE"){
  g1 <- ggplot( data.frame(ridge.mod$lambda), aes( x = ridge.mod$lambda, y = (ridge.mod$cvm))) + 
    geom_point() + 
    geom_vline( xintercept = smallestLambda, col = "blue" , lty = 2 ) + 
    labs(caption = paste("Test MSE values for Different Tuning Parameters. Smallest MSE at lambda = ", 
                         smallestLambda), title = title, y = "Test MSE", x = "Tuning Parameter")
  
  }
  
  if(metric == "RMSE"){
  g1 <- ggplot( data.frame(ridge.mod$lambda), aes( x = ridge.mod$lambda, y = sqrt(ridge.mod$cvm))) + 
    geom_point() + 
    geom_vline( xintercept = smallestLambda, col = "blue" , lty = 2 ) + 
    labs(caption = paste("Test RMSE values for Different Tuning Parameters. Smallest RMSE at lambda = ", smallestLambda), 
         title = title, y = "Test RMSE", x = "Tuning Parameter")
  }
  
  g1
}
```

```{r echo=FALSE}
#We're using 10-fold CV in the cv.glmnet() command below, so we need to set a seed
set.seed(1)

#Construct XD
 XD <- model.matrix(Price ~ ., data = VRBO)

#Use loop to look at values of lambda, plot
ridge.mod <- cv.glmnet(XD[,-1], 
                       VRBO$Price,
                       alpha = 0, 
                       lambda = seq(from = 0, to = 25, by = 0.1), 
                       standardize = TRUE)

ridgePlot(ridge.mod, metric = "RMSE",
          title = "Figure 3.2: RMSE of VRBO Price Model (Ridge Regression)")

#Output model using best value of lambda
```

```{r eval=FALSE, include=FALSE}
ridge.mod$lambda.min # lambda associated with smallest RMSE
sqrt(min(ridge.mod$cvm)) # value of smallest RMSE
```

The $\boldsymbol{\hat\beta}$ coefficients estimated using $\lambda_{Ridge} =$ 14.9 are given below in Table 3.1. The model we build using these coefficients and $\lambda_{Ridge}$ = 14.9 has an RMSE of 61.035, meaning that the model is incorrect in its price predictions by 61.04 USD, on average. A lower RMSE would be preferable, given that the middle 50% of rentals in the data set are priced between 55 and 130 USD per night, and that less than 8.3% of the properties in the data set exceed 200 USD per night. An average error of 61.04 USD seems likely to impact future pricing choices substantially.

In the next section, we will determine whether the lasso method gives us a model with a (preferably) smaller RMSE. 


```{r echo=FALSE}
#Ridge model using lambda = 14.9
ridgem <- glmnet(XD[,-1], 
                       VRBO$Price,
                       alpha = 0, 
                       lambda = 14.9, 
                       standardize = TRUE)
  
#Make a data frame holding the ridge model coefficients
ridge.betas <- as.numeric(coefficients(ridgem))
RBetas <- data.frame("Coefficient Estimates" = ridge.betas)
rownames(RBetas) <- colnames(XD)

knitr::kable(RBetas, 
             caption = "Table 3.1: Coefficient Estimates (Ridge Regression Model)", col.names = c("Coefficient Estimate")) %>%
  kable_styling()

```

## Part 4: Lasso Method

In this section, we briefly explain the motivation for and process of using the lasso method, then train our second model using this technique.

The lasso method is somewhat similar to ridge regression: it seeks to minimize the sum of squared errors as well as a penalty term. The lasso penalty term is $\boldsymbol{\lambda_{Lasso}|\hat{\beta}|}$, which differs from that of ridge regression in that it uses the absolute value of $\boldsymbol{\hat\beta}$, rather than $\boldsymbol{\hat\beta^T\hat\beta}$. Unlike ridge regression, the lasso method can shrink $\boldsymbol{\hat\beta}$ coefficients to 0, which eliminates the features associated with those coefficients from the model. The lasso method thus offers feature selection as well as shrinkage (and, in fact, favors selection over shrinkage when the $\boldsymbol{\hat\beta}$ coefficients become small). 

The process of and rationale behind choosing the tuning parameter, $\lambda_{Lasso}$, are the same as that of choosing $\lambda_{Ridge}$ described in Part 3. We again consider a range of non-negative values for $\lambda_{Lasso}$, beginning with $0$, in order to determine which value leads to the lowest root mean squared error (RMSE). We will here consider values of $\lambda_{Lasso}$ between 0 and 10 in increments of 0.1 (decreasing the range we used in Part 3 in order to increase the legibility of Figure 4.1). As in Part 3, we then estimate the coefficients $\boldsymbol{\hat\beta}$ using each value of $\lambda_{Lasso}$, and then use 10-fold cross validation to find the test RMSE of each of those models. 

Figure 4.1 shows that the lowest RMSE (61.433) occurs when $\lambda_{Lasso} =$ 0.5. The RMSE obtained using lasso is slightly higher than the RMSE obtained using ridge regression (which was 61.035); the lasso method model offers slightly lower predictive accuracy than the ridge regression model. The lasso model is incorrect in its rental price predictions by 61.43 USD, on average.

```{r eval=FALSE, include=FALSE}
# Testing to find good representative range for lambda_lasso tuning parameter
# Adapt ridge process, setting alpha to 1
set.seed(1)

# Use loop to look at values of lambda, plot
lasso.mod <- cv.glmnet(XD[,-1], 
                       VRBO$Price,
                       alpha = 1, 
                       lambda = seq(from = 0, to = 25, by = 0.1), 
                       standardize = TRUE)

ridgePlot(lasso.mod, metric = "RMSE",
          title = "Figure 4.1 (testing range): RMSE of VRBO Price Model (Lasso Regression)")
```

```{r echo=FALSE}
#Final choice of range to look at for lambda_lasso
set.seed(1)

lasso.mod <- cv.glmnet(XD[,-1], 
                       VRBO$Price,
                       alpha = 1, 
                       lambda = seq(from = 0, to = 10, by = 0.1), 
                       standardize = TRUE)

ridgePlot(lasso.mod, metric = "RMSE",
          title = "Figure 4.1: RMSE of VRBO Price Model (Lasso Regression)")
```

```{r eval=FALSE, include=FALSE}
lasso.mod$lambda.min # lambda associated with smallest RMSE (lasso)
sqrt(min(lasso.mod$cvm)) # value of smallest RMSE (lasso)
```

We note that the lasso model contains fewer features than the ridge regression model. Table 4.1 lists the coefficient estimates from the lasso model we obtain by setting $\lambda_{Lasso} =$ 0.5. Lasso shrank a number of the coefficient estimates to 0, eliminating the following features from the model: neighorhoods Archer Heights, Burnside, Edison Park, Gage Park, Hegewisch, Hermosa, Hyde Park, Kenwood, McKinley Park, Morgan Park, Portage Park, South Chicago, and West Elsdon; all districts except Far Southeast; walking score; and percentage of properties in the neighborhood that are also rental properties rather than permanent residences. These eliminated features are likely highly correlated with other features that the lasso process retained in the model (as shown in Figure 3.1, walking score is highly correlated with bike score and transit score, for example; the lasso model retained bike score and transit score).

In the next section, we will build a model using a third and final method, elastic net, and determine whether it offers better predictive accuracy than the models built using ridge regression and the lasso method. 

```{r echo=FALSE}
#Lasso model using lambda = 0.5
lassom <- glmnet(XD[,-1], 
                       VRBO$Price,
                       alpha = 1, 
                       lambda = 0.5, 
                       standardize = TRUE)
  
#Make a data frame holding the coefficients
lasso.betas <- as.numeric(coefficients(lassom))
LBetas <- data.frame("Coefficient Estimates" = lasso.betas)
rownames(LBetas) <- colnames(XD)

knitr::kable(LBetas, 
             caption = "Table 4.1: Coefficient Estimates (Lasso Regression Model)", col.names = c("Coefficient Estimate")) %>%
  kable_styling()
```

## Part 5: Elastic Net Method

This section briefly explains the motivation for and process of using the elastic net method, then trains a third and final model using the technique.

Like ridge regression and the lasso method, the elastic net method estimates the $\boldsymbol{\hat\beta}$ coefficients by minimizing the residual sum of squares in addition to a penalty term. The elastic net penalty term incorporates elements of both the ridge regression and the lasso penalty terms: 

$$\lambda_{Elastic}\Sigma((1 - \alpha)\hat{\beta_j^2} + \alpha|\hat{\beta_j}|)$$

As with ridge regression and the lasso method, we seek the value of $\lambda_{Elastic}$ that minimizes the penalty term and RSS, which requires consideration of a second tuning parameter: $\alpha$, which can take any value from 0 to 1. When $\alpha$ nears 0, the resulting model will more closely resemble a ridge regression model; when $\alpha$ nears 1, the resulting model will more closely resemble a lasso model. The parameter $\alpha$ offers a way of balancing the shrinkage provided by ridge regression and the tendency toward selection in the lasso method.

We choose values of $\alpha$ and $\lambda_{Elastic}$ by considering all values of $\alpha$ from 0 to 1 in increments of 0.01. For each increment of $\alpha$, we consider a range of values for $\lambda_{Elastic}$ between 0 and 25 in increments of 0.1. (This range was chosen because $\lambda \in$ [0, 25] is the more expansive of the two ranges used in finding optimizing values of $\lambda_{Ridge}$ and $\lambda_{Lasso}$ in Parts 3 and 4, respectively.) 

For each value of $\alpha$, we record the value of $\lambda_{Elastic}$ that generated $\boldsymbol{\hat\beta}$ coefficients that resulted in the model with the lowest RMSE, and also record that RMSE. When we have obtained such values of $\lambda_{Elastic}$ and an associated RMSE for each value of $\alpha$, we select the values of $\alpha$ and $\lambda_{Elastic}$ that result in the lowest RMSE and use them to build the final model. 

Table 5.1 displays the values of $\alpha$, along with the associated $\lambda_{Elastic}$ that led to the lowest possible RMSE, and that RMSE. Of the parameter choices in this table, $\alpha =$ 0.01 and $\lambda_{Elastic} =$ 14 result in the model with the overall lowest RMSE (61.033). 

We choose these parameter values to construct the final model, which is incorrect in its rental price predictions by 61.03 USD, on average.

```{r include=FALSE}
# Choose sequence of values for alpha 
alphaseq <- seq(from = 0, to = 1 , by = 0.01)

storage <- data.frame("Alpha" = rep(NA,length(alphaseq)), "Lambda" = rep(NA,length(alphaseq)), "RMSE" = rep(NA,length(alphaseq)))

a = 1 
# Run 10-fold CV
for( i in alphaseq ){
  set.seed(1)
  cv.elastic <- cv.glmnet(XD[,-1], VRBO$Price, alpha = i,lambda = seq(from = 0, to = 25, by = 0.1))
  storage$Lambda[a] <- cv.elastic$lambda.min
  storage$RMSE[a] <- sqrt(min(cv.elastic$cvm))
  storage$Alpha[a] <- i
  a = a + 1 
}
```

```{r echo=FALSE}
# Table of tuning parameters and RMSE
knitr::kable(storage, caption = "Table 5.1: Parameters and RMSE via Elastic Net") %>%
  kable_styling()
```

```{r eval=FALSE, include=FALSE}
storage[which.min(storage$RMSE),]
```

Table 5.2 displays the $\boldsymbol{\hat\beta}$ coefficients for the ridge regression model constructed in Part 3, the lasso method model constructed in Part 4, and the elastic net model constructed in this section. 

The elastic net model shrank fewer $\boldsymbol{\hat\beta}$ coefficients to 0 than did the lasso method, retaining 53 non-zero $\boldsymbol{\hat\beta}$ coefficients, as opposed to the lasso model's 40 non-zero coefficients. (The ridge regression model, as noted in Part 3, did not shrink any of the 62 total coefficients to 0.) Features omitted in the lasso model but retained in the elastic net model are: neighorhoods Edison Park, Hegewisch, Hermosa, Kenwood, and Portage Park; all districts except West; walking score; and percentage of properties in the neighborhood that are also rental properties.

```{r echo=FALSE}
# Obtaining coefficients

# Train Elastic Net
elastic.final <- glmnet(XD[,-1], VRBO$Price, alpha = 0.01,
                      lambda = 14)

# Store the coefficients
elastic.betas <- as.numeric(coefficients(elastic.final))

# Create a data frame
Betas <- data.frame("Ridge" = ridge.betas, "Lasso" = lasso.betas, "Elastic" = elastic.betas)

rownames(Betas) <- colnames(XD)

knitr::kable(Betas, caption = "Table 5.2: Comparison of Coefficient Estimates (Three Models)") %>%
  kable_styling()
```

```{r eval=FALSE, include=FALSE}
#Calculating numbers of non-zero betas
dim(Betas)
# No beta coefficients will be 0 for ridge regression
62 - nrow(Betas[Betas$Lasso==0,]) # number of non-zero lasso model coefficients
62 - nrow(Betas[Betas$Elastic==0,]) # number of non-zero elastic model coefficients
```


## Part 6: Comparison and Conclusions

The three models constructed in this report perform very similarly with respect to predictive accuracy. The values of the tuning parameters for each, as well as the RMSE of each, are given in Table 6.1.

The lowest test RMSE (61.033) is achieved by the elastic net model. This model retained 53 of the 62 total possible coefficients to be estimated. The $\alpha$ value, 0.01, indicates that this model behaves more similarly to a ridge regression model than to a lasso model; we also observe that the value of $\lambda_{Elastic} =$ 14 is much closer to $\lambda_{Ridge} =$ 14.9 than to $\lambda_{Lasso} =$ 0.5. The second best predictive performance was that of the ridge regression model, with an RMSE of 61.035 USD, and the worst performance of the three was that of the lasso model, with an RMSE of 61.433 USD. 

These RMSEs do not differ substantially from one another; the percentage difference in RMSE between the elastic net model and ridge regression model is very close to 0. The difference in RMSE between the elastic net model and the lasso model was 0.652 percent, also very small. If VRBO wishes to make a decision based purely on predictive performance, they should choose the elastic net model; however, the minuscule difference in RMSE and the high RMSE (relative to the rental prices in the data set) of all three of the model types requested by the client suggest that exploring models beyond ridge regression, lasso, and elastic net may be advantageous.

```{r eval=FALSE, include=FALSE}
# Percentage difference between RMSEs: elastic vs ridge
(61.0351 - 61.0327)/61.0351

# Percentage difference between RMSEs: elastic vs lasso
(61.4330 - 61.0327)/61.4330
```

```{r echo=FALSE}
Method <- c("Ridge", "Lasso", "Elastic Net")
TuningParameters <- c("lambda = 14.9", "lambda = 0.5", "lambda = 14, alpha = 0.01")
TestRMSE <- c(61.0351, 61.4330, 61.0327)
Non0Coefficients <- c(62, 40, 53)
Table6.1 <- cbind(Method, TuningParameters, TestRMSE, Non0Coefficients)

knitr::kable(Table6.1, caption = "Table 6.1: Comparison of Models", col.names = c("Method", "Tuning Parameters", "Test RMSE", "Non-0 Coefficients")) %>%
  kable_styling()
```

